Automatically generated by Mendeley Desktop 1.17.2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop
@article{Drchal2012,
doi = {10.1145/2330163.2330241},
isbn = {9781450311779},
journal = {Proceedings of the fourteenth international conference on Genetic and evolutionary computation conference GECCO 12},
pages = {545},
title = {{Distance measures for HyperGP with fitness sharing}},
year = {2012}
}
@techreport{Angus1991,
author = {Angus, J. E.},
file = {:home/alex/Downloads/a247725.pdf:pdf},
institution = {DTIC Document},
number = {91-16},
title = {{Criteria for Choosing the Best Neural Network: Part 1}},
year = {1991}
}
@article{Akaike1973,
abstract = {In this paper it is shown that the classical maximum likelihood principle can be considered to be a method of asymptotic realization of an optimum estimate with respect to a very general information theoretic criterion. This observation shows an extension of the principle to provide answers to many practical problems of statistical model fitting.},
author = {Akaike, H},
doi = {10.1016/j.econlet.2011.12.027},
file = {:home/alex/Downloads/10.1007@978-1-4612-1694-015.pdf:pdf},
isbn = {9780387983554},
issn = {00493848},
journal = {International Symposium on Information Theory},
pages = {267--281},
pmid = {217},
title = {{Information theory and an extension of the maximum likelihood principle}},
year = {1973}
}
@article{kolmogorov22representation,
author = {Kolmogorov, A N},
journal = {MR},
pages = {2669},
title = {{On the representation of continuous functions of many variables by superpositions of continuous functions of one variable and addition}},
volume = {22},
year = {1957}
}
@article{Efe2008,
author = {Efe, M. \"{O}},
isbn = {1106300890},
issn = {13704621},
journal = {Neural Processing Letters},
keywords = {Activation functions,Dynamical system identification,Levenberg-Marquardt algorithm},
number = {2},
pages = {63--79},
title = {{Novel neuronal activation functions for feedforward neural networks}},
volume = {28},
year = {2008}
}

@incollection{Taras2014,
author = {Taras, Kowaliw and Nicolas, Bredeche and Doursat, Ren{\`{e}}},
booktitle = {Studies in Computational Intelligence},
doi = {10.1007/978-3-642-55337-0},
file = {:home/alex/Downloads/chp{\%}3A10.1007{\%}2F978-3-642-55337-0{\_}5.pdf:pdf},
isbn = {9783642553363},
issn = {1860949X},
pages = {159--185},
title = {{HyperNEAT: The First Five Years}},
volume = {557},
year = {2014}
}
@article{Moriarty1996,
abstract = {This article demonstrates the advantages of a cooperative, coevolutionary search in difficult control problems. The symbiotic adaptive neuroevolution (SANE) system coevolves a population of neurons that cooperate to form a functioning neural network. In this process, neurons assume different but overlapping roles, resulting in a robust encoding of control behavior. SANE is shown to be more efficient and more adaptive and to maintain higher levels of diversity than the more common network-based population approaches. Further empirical studies illustrate the emergent neuron specializations and the different roles the neurons assume in the population.},
author = {Moriarty, David E. and Miikkulainen, Risto},
file = {:home/alex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Moriarty, Miikkulainen - 1996 - Efficient reinforcement learning through symbiotic evolution.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {genetic algorithms,neural networks,neuro-evolution,reinforcement learning},
number = {1-3},
pages = {11--32},
pmid = {10021764},
title = {{Efficient reinforcement learning through symbiotic evolution}},
volume = {22},
year = {1996}
}
@article{Stanley2009,
abstract = {Research in neuroevolution-that is, evolving artificial neural networks (ANNs) through evolutionary algorithms-is inspired by the evolution of biological brains, which can contain trillions of connections. Yet while neuroevolution has produced successful results, the scale of natural brains remains far beyond reach. This article presents a method called hypercube-based NeuroEvolution of Augmenting Topologies (HyperNEAT) that aims to narrow this gap. HyperNEAT employs an indirect encoding called connective compositional pattern-producing networks (CPPNs) that can produce connectivity patterns with symmetries and repeating motifs by interpreting spatial patterns generated within a hypercube as connectivity patterns in a lower-dimensional space. This approach can exploit the geometry of the task by mapping its regularities onto the topology of the network, thereby shifting problem difficulty away from dimensionality to the underlying problem structure. Furthermore, connective CPPNs can represent the same connectivity pattern at any resolution, allowing ANNs to scale to new numbers of inputs and outputs without further evolution. HyperNEAT is demonstrated through visual discrimination and food-gathering tasks, including successful visual discrimination networks containing over eight million connections. The main conclusion is that the ability to explore the space of regular connectivity patterns opens up a new class of complex high-dimensional tasks to neuroevolution.},
author = {Stanley, Kenneth O and D'Ambrosio, David B and Gauci, Jason},
doi = {10.1162/artl.2009.15.2.15202},
file = {:home/alex/Downloads/stanley3{\_}{\_}.pdf:pdf},
isbn = {1064-5462 (Print)$\backslash$r1064-5462 (Linking)},
issn = {1064-5462},
journal = {Artificial Life},
keywords = {Biological Evolution,Computational Biology,Food,HyperNEAT,Nerve Net,Nerve Net: metabolism,Visual Perception,Visual Perception: physiology},
number = {2},
pages = {185--212},
pmid = {19199382},
title = {{A hypercube-based encoding for evolving large-scale neural networks.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19199382},
volume = {15},
year = {2009}
}
@article{Phillips1962,
author = {Phillips, David L.},
isbn = {0004-5411},
issn = {00045411},
journal = {Journal of the ACM},
number = {1},
pages = {84--97},
title = {{A Technique for the Numerical Solution of Certain Integral Equations of the First Kind}},
volume = {9},
year = {1962}
}
@article{Maul2014,
author = {Maul, Tomas H. and Bargiela, Andrzej and Yew, Chong Siang and Adamu, Abdullahi S.},
file = {:home/alex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Maul et al. - 2014 - Towards Evolutionary Deep Neural Networks.pdf:pdf},
isbn = {9780956494481},
journal = {ECMS},
keywords = {cooperative coevolution,deep learning,neuroevolution},
title = {{Towards Evolutionary Deep Neural Networks}},
year = {2014}
}
@article{Turner2014,
abstract = {NeuroEvolution is the application of Evolu- tionary Algorithms to the training of Artificial Neural Networks. Currently the vast majority of NeuroEvolution- ary methods create homogeneous networks of user defined transfer functions. This is despite NeuroEvolution being capable of creating heterogeneous networks where each neuron's transfer function is not chosen by the user, but selected or optimised during evolution. This paper dem- onstrates how NeuroEvolution can be used to select or optimise each neuron's transfer function and empirically shows that doing so significantly aids training. This result is important as the majority of NeuroEvolutionary methods are capable of creating heterogeneous networks using the methods described.},
author = {Turner, Andrew James and Miller, Julian Francis},
doi = {10.1007/s12065-014-0115-5},
file = {:home/alex/Downloads/NeuroEvolution{\_}Evolving{\_}Heterogeneous{\_}Ar.pdf:pdf},
issn = {18645917},
journal = {Evolutionary Intelligence},
keywords = {Artificial Neural Networks,Cartesian Genetic Programming,Computational intelligence,Evolutionary Algorithms,Heterogeneous Artificial Neural Networks,NeuroEvolution},
number = {3},
pages = {135--154},
title = {{NeuroEvolution: Evolving Heterogeneous Artificial Neural Networks}},
volume = {7},
year = {2014}
}
@article{Laudani2015,
author = {Laudani, Antonino and Lozito, Gabriele Maria and Fulginei, Francesco Riganti and Salvini, Alessandro},
file = {:home/alex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Laudani et al. - 2015 - On training efficiency and computational costs of a feed forward neural network A review.pdf:pdf},
issn = {16875273},
journal = {Computational Intelligence and Neuroscience},
publisher = {Hindawi Publishing Corporation},
title = {{On training efficiency and computational costs of a feed forward neural network: A review}},
volume = {2015},
year = {2015}
}
@article{Karlik2010,
abstract = {The activation function used to transform the activati on level of a unit (neuron) into an output signal. There are a number of common activation functi ons in use with artificial neural networks (ANN). The most common choice of activation functions fo r multi layered perceptron (MLP) is used as transfer functions in research and engineerin g. Among the reasons for this popularity are its boundedness in the unit interval, the function's and its derivative's fast computability, and a number of amenable mathematical properties in the re alm of approximation theory. However, considering the huge variety of problem domains MLP i s applied in, it is intriguing to suspect that specific problems call for single or a set of specific a ctivation functions. The aim of this study is to analyze the performance of generalized MLP architectures which has back-propagation algorithm using various different activation functions for the neurons of hidden and output layers. For experimental comparisons, Bi-polar sigmoid, Uni-pola r sigmoid, Tanh, Conic Section, and Radial Bases Function (RBF) were used.},
author = {Karlik, Bekir and Olgac, Av},
file = {:home/alex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Karlik, Olgac - 2010 - Performance analysis of various activation functions in generalized MLP architectures of neural networks.pdf:pdf},
issn = {2180124X},
journal = {International Journal of Artificial Intelligence and Expert Systems (IJAE)},
keywords = {BAD,activation functions,multi layered perceptron,neural networks,performance analysis},
mendeley-tags = {BAD},
number = {4},
pages = {111--122},
title = {{Performance analysis of various activation functions in generalized MLP architectures of neural networks}},
url = {http://www.cscjournals.org/csc/manuscript/Journals/IJAE/volume1/Issue4/IJAE-26.pdf},
volume = {1},
year = {2010}
}
@article{Huber1995,
author = {Huber, Reinhold and Mayer, H.A. and Schwaiger, Roland},
file = {:home/alex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huber, Mayer, Schwaiger - 1995 - netGEN-A Parallel System Generating Problem-Adapted Topologies of Artificial Neural Networks by means o.pdf:pdf},
title = {{netGEN-A Parallel System Generating Problem-Adapted Topologies of Artificial Neural Networks by means of Genetic Algorithms}},
volume = {7},
year = {1995}
}
@article{Yao1997,
author = {Yao, X and Liu, Y},
file = {:home/alex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yao, Liu - 1997 - A new evolutionary system for evolving artificial neural networks.pdf:pdf},
issn = {1045-9227},
journal = {IEEE transactions on neural networks / a publication of the IEEE Neural Networks Council},
keywords = {Artificial neural networks,Australia,Benchmark testing,EPNet,Evolutionary computation,Genetic mutations,Genetic programming,Machine learning,Medical diagnosis,Medical tests,Noise reduction,architectures,behavior evolution,connection weights,evolutionary programming,evolving neural networks,feedforward neural nets,feedforward neural networks,generalisation,generalisation (artificial intelligence),learning (artificial intelligence),machine learning,mutation operators,neural net architecture,node splitting,partial training},
number = {3},
pages = {694--713},
pmid = {18255671},
title = {{A new evolutionary system for evolving artificial neural networks.}},
volume = {8},
year = {1997}
}
@article{Hornik1989,
abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators. ?? 1989.},
author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
file = {:home/alex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hornik, Stinchcombe, White - 1989 - Multilayer feedforward networks are universal approximators.pdf:pdf},
isbn = {08936080 (ISSN)},
issn = {08936080},
journal = {Neural Networks},
keywords = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation},
number = {5},
pages = {359--366},
pmid = {74},
title = {{Multilayer feedforward networks are universal approximators}},
volume = {2},
year = {1989}
}
@article{Sebald1998,
author = {Sebald, A V and Chellapilla, K},
file = {:home/alex/Downloads/10.1.1.49.7245.pdf:pdf},
isbn = {3540648917},
issn = {16113349},
journal = {Evolutionary {\{}P{\}}rogramming {\{}VII{\}}},
pages = {271--280},
title = {{On Making Problems Evolutionarily Friendly -- Part 1: Evolving the Most Convenient Representations}},
year = {1998}
}
@article{Gomez1998,
abstract = {The success of evolutionary methods on standard control learning tasks has created a need for new benchmarks. The classic pole balancing problem is no longer difficult enough to serve as a viable yardstick for measuring the learning efficiency of these systems. In this paper we present a more difficult version to the classic problem where the cart and pole can move in a plane. We demonstrate a neuroevolution system (Enforced Sub-Populations, or ESP) that can solve this difficult problem without velocity information.},
author = {Gomez, Faustino and Miikkulainen, Risto},
file = {:home/alex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gomez, Miikkulainen - 1998 - 2-D pole balancing with recurrent evolutionary networks(3).pdf:pdf},
journal = {International Conference on Artificial Neural Networks},
pages = {2--7},
title = {{2-D pole balancing with recurrent evolutionary networks}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.57.5054},
year = {1998}
}
@article{Liu1996,
abstract = {Evolutionary design of artificial neural networks (ANNs) offers a very promising and automatic alternative to designing ANNs manually. The advantage of evolutionary design over the manual design is their adaptability to a dynamic environment. Most research in evolving ANNs only deals with the topological structure of ANNs and little has been done on the evolution of both topological structures and node transfer functions. The paper presents a new automatic method to design general neural networks (GNNs) with different nodes. GNNs combine generalisation capabilities of distributed neural networks (DNNs) and computational efficiency of local neural networks (LNNs). We use an evolutionary programming (EP) algorithm with new mutation operators which are very effective for evolving GNN architectures and weights simultaneously. Our EP algorithm allows GNNs to grow as well as shrink during the evolutionary process. Our experiment results show the effectiveness and accuracy of evolved GNNs},
author = {Liu, Y and Yao, X},
file = {:home/alex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu, Yao - 1996 - Evolutionary design of artificial neural networks with different nodes.pdf:pdf},
isbn = {0-7803-2902-3},
journal = {Proceedings of IEEE International Conference on Evolutionary Computation},
keywords = {pid controller,simplified ca,trms system},
pages = {913--917},
title = {{Evolutionary design of artificial neural networks with different nodes}},
year = {1996}
}
@article{Hwang1997,
author = {Hwang, Min Woong and Choi, Jin Young and Park, Jaehong},
file = {:home/alex/Downloads/10.1109@icec.1997.592399.pdf:pdf},
isbn = {0780339495},
journal = {Evolutionary Computation, 1997., IEEE International Conference on},
pages = {667--671},
title = {{Evolutionary projection neural networks}},
year = {1997}
}
@inproceedings{Krishnan1994,
author = {Krishnan, Rajendra and Ciesielski, Victor B},
booktitle = {Proceedings of the Australian Conference on Neural Networks},
file = {:home/alex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krishnan, Ciesielski - 1994 - 2DELTA-GANN A New Approach to Training Neural Networks Using Genetic Algorithms.pdf:pdf},
keywords = {bibtex-import,nn},
pages = {194--197},
title = {{2DELTA-GANN: A New Approach to Training Neural Networks Using Genetic Algorithms}},
year = {1994}
}
@article{Stanley2002a,
abstract = {The ventral visual pathway implements object recognition and categorization in a hierarchy of processing areas with neuronal selectivities of increasing complexity. The presence of massive feedback connections within this hierarchy raises the possibility that normal visual processing relies on the use of computational loops. It is not known, however, whether object recognition can be performed at all without such loops (i.e., in a purely feed-forward mode). By analyzing the time course of reaction times in a masked natural scene categorization paradigm, we show that the human visual system can generate selective motor responses based on a single feed-forward pass. We confirm these results using a more constrained letter discrimination task, in which the rapid succession of a target and mask is actually perceived as a distractor. We show that a masked stimulus presented for only 26 msecand often not consciously perceivedcan fully determine the earliest selective motor responses: The neural representations of the stimulus and mask are thus kept separated during a short period corresponding to the feedforward sweep. Therefore, feedback loops do not appear to be mandatory for visual processing. Rather, we found that such loops allow the masked stimulus to reverberate in the visual system and affect behavior for nearly 150 msec after the feed-forward sweep.},
author = {Stanley, Kenneth O. and Miikkulainen, Risto},
file = {:home/alex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Stanley, Miikkulainen - 2002 - Evolving Neural Networks through Augmenting Topologies(2).pdf:pdf},
isbn = {1063-6560},
issn = {1063-6560},
journal = {Evolutionary Computation},
number = {2},
pages = {99--127},
pmid = {12180173},
title = {{Evolving Neural Networks through Augmenting Topologies}},
volume = {10},
year = {2002}
}
@article{Miller2000,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Miller, Julian F. and Thomson, Peter},
eprint = {arXiv:1011.1669v3},
file = {:home/alex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Miller, Thomson - 2000 - Cartesian genetic programming.pdf:pdf},
isbn = {3540673393},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {121--132},
pmid = {25246403},
title = {{Cartesian genetic programming}},
volume = {1802},
year = {2000}
}
@article{Dolinsky2010,
author = {Dolinsky, Jan and Konishi, Sadanori},
file = {:home/alex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dolinsky, Konishi - 2010 - Echo State Networks with Non-monotonous Activation Functions.pdf:pdf},
journal = {Japanese Joint Statistical Meeting},
keywords = {0001},
mendeley-tags = {0001},
pages = {10},
title = {{Echo State Networks with Non-monotonous Activation Functions}},
year = {2010}
}
@article{Gomez1997,
author = {Gomez, F. J. and Miikkulainen, R.},
file = {:home/alex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gomez, Miikkulainen - 1997 - Incremental Evolution of Complex General Behavior(2).pdf:pdf},
isbn = {1059-7123},
issn = {10597123},
journal = {Adaptive Behavior},
keywords = {environmental complexification},
number = {3-4},
pages = {317--342},
title = {{Incremental Evolution of Complex General Behavior}},
volume = {5},
year = {1997}
}
@article{Agostinelli2015,
archivePrefix = {arXiv},
arxivId = {1412.6830},
author = {Agostinelli, Forest and Hoffman, Matthew and Sadowski, Peter and Baldi, Pierre},
eprint = {1412.6830},
file = {:home/alex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Agostinelli et al. - 2015 - Learning Activation Functions to Improve Deep Neural Networks.pdf:pdf},
isbn = {9783642352881},
issn = {01628828},
journal = {Proceedings of the 2015 International Conference on Learning Representations (ICLR'15)},
number = {2013},
pages = {1--9},
pmid = {8514134},
title = {{Learning Activation Functions to Improve Deep Neural Networks}},
url = {http://arxiv.org/abs/1412.6830},
year = {2015}
}
@inproceedings{Braun1994,
author = {Braun, Heinrich and Zagorski, Peter},
booktitle = {Parallel Problem Solving from Nature -- PPSN III},
file = {:home/alex/Downloads/chp{\%}3A10.1007{\%}2F3-540-58484-6{\_}287.pdf:pdf},
pages = {440--451},
title = {{ENZO-M - A Hybrid Approach for Optimizing Neural Networks by Evolution and Learning}},
year = {1994}
}
@article{Technology2013,
abstract = {The Back propagation algorithm allows multilayer feed forward neural networks to learn input/output mappings from training samples. Back propagation networks adapt itself to learn the relationship between the set of example patterns, and could be able to apply the same relationship to new input patterns. The network should be able to focus on the features of an arbitrary input. The activation function is used to transform the activation level of a unit (neuron) into an output signal. There are a number of common activation functions in use with artificial neural networks (ANN). Our paper aims to perform analysis of the different activation functions and provide a benchmark of it. The purpose is to figure out the optimal activation function for a problem.},
author = {Sibi, P and Jones, S and Siddarth, P},
file = {:home/alex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sibi, Jones, Siddarth - 2013 - Analysis of different activation functions using back propagation neural networks.pdf:pdf},
issn = {1817-3195},
journal = {Journal of theoretical and applied Information Technology},
keywords = {activation function,ann,artificial neural network,back propagation network,bpn},
number = {3},
pages = {1264--1268},
title = {{Analysis of different activation functions using back propagation neural networks}},
volume = {47},
year = {2013}
}
@article{Morse2016,
author = {Morse, Gregory and Stanley, Kenneth O},
doi = {10.1145/2908812.2908916},
file = {:home/alex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Morse, Stanley - 2016 - Simple Evolutionary Optimization Can Rival Stochastic Gradient Descent in Neural Networks.pdf:pdf},
isbn = {9781450342063},
keywords = {all or part of,artificial,deep learning,intelligence,machine learning,neural networks,or,or hard copies of,pattern recognition and classification,permission to make digital,this work for personal},
number = {Gecco},
title = {{Simple Evolutionary Optimization Can Rival Stochastic Gradient Descent in Neural Networks}},
year = {2016}
}
@article{Doolan2015,
author = {Doolan, Timothy F},
file = {:home/alex/Downloads/f1248069366.pdf:pdf},
title = {{MSc Artificial Intelligence Master Thesis Using indirect encoding in neural networks to solve supervised machine learning tasks by}},
year = {2015}
}
@article{Pujol1998,
abstract = {Evolutionary computation is a class of global search$\backslash$ntechniques based on the learning process of a$\backslash$npopulation of potential solutions to a given problem,$\backslash$nthat has been successfully applied to a variety of$\backslash$nproblems. In this paper a new approach to the$\backslash$nconstruction of neural networks based on evolutionary$\backslash$ncomputation is presented. A linear chromosome combined$\backslash$nto a graph representation of the network are used by$\backslash$ngenetic operators, which allow the evolution of the$\backslash$narchitecture and the weights simultaneously without the$\backslash$nneed of local weight optimization. This paper describes$\backslash$nthe approach, the operators and reports results of the$\backslash$napplication of this technique to several binary$\backslash$nclassification problems.},
author = {Pujol, Joao Carlos Figueira and Poli, Riccardo},
doi = {doi:10.1023/A:1008272615525},
file = {:home/alex/Downloads/art{\%}3A10.1023{\%}2FA{\%}3A1008272615525.pdf:pdf},
journal = {Applied Intelligence},
keywords = {Evolutionary computation,PDGP,genetic algorithms,genetic programming,neural networks},
pages = {73--84},
title = {{Evolution of the Topology and the Weights of Neural Networks using Genetic Programming with a Dual Representation}},
url = {http://citeseer.ist.psu.edu/322291.html},
volume = {8},
year = {1998}
}
@article{Chen2016,
abstract = {In the neuroevolution literature, research has primarily focused on evolving the number of nodes, connections, and weights in artificial neural networks. Few attempts have been made to evolve activation functions. Research in evolving activation functions has mainly focused on evolving function parameters, and developing heterogeneous networks by selecting from a fixed pool of activation functions. This paper introduces a novel technique for evolving heterogeneous artificial neural networks through combinatorially generating piecewise activation functions to enhance expressive power. I demonstrate this technique on NeuroEvolution of Augmenting Topologies using ArcTan and Sigmoid, and show that it outperforms the original algorithm on non-Markovian double pole balancing. This technique expands the landscape of unconventional activation functions by demonstrating that they are competitive with canonical choices, and introduces a purview for further exploration of automatic model selection for artificial neural networks.},
archivePrefix = {arXiv},
arxivId = {1605.05216},
author = {Chen, Justin},
eprint = {1605.05216},
file = {:home/alex/Downloads/1605.05216v1.pdf:pdf},
title = {{Combinatorially Generated Piecewise Activation Functions}},
url = {http://arxiv.org/abs/1605.05216},
year = {2016}
}
@article{Verbancsics2011,
abstract = {A challenging goal of generative and developmental systems (GDS) is to effectively evolve neural networks as complex and capable as those found in nature. Two key properties of neural structures in nature are regularity and modularity. While HyperNEAT has proven capable of generating neural network connectivity patterns with regularities, its ability to evolve modularity remains in question. This paper investigates how altering the traditional approach to determining whether connections are expressed in HyperNEAT influences modularity. In particular, an extension is introduced called a Link Expression Output (HyperNEAT-LEO) that allows HyperNEAT to evolve the pattern of weights indepen- dently from the pattern of connection expression. Because HyperNEAT evolves such patterns as functions of geometry, important general topographic principles for organizing connectivity can be seeded into the initial population. For example, a key topographic concept in nature that encour- ages modularity is locality, that is, components of a module are located near each other. As experiments in this paper show, by seeding HyperNEAT with a bias towards local connectivity implemented through the LEO, modular structures arise naturally. Thus this paper provides an important clue to how an indirect encoding of network structure can be encouraged to evolve modularity.},
author = {Verbancsics, Phillip and Stanley, Kenneth O},
doi = {10.1145/2001576.2001776},
file = {:home/alex/Downloads/p1483-verbancsics.pdf:pdf},
isbn = {9781450305570},
journal = {Proceedings of the 13th annual conference on Genetic and evolutionary computation - GECCO '11},
pages = {1483},
title = {{Constraining connectivity to encourage modularity in HyperNEAT}},
url = {http://portal.acm.org/citation.cfm?doid=2001576.2001776},
year = {2011}
}
@inproceedings{6934819,
author = {{\"{O}}zveren, C S and Sapeluk, A T and Birch, A},
booktitle = {2014 49th International Universities Power Engineering Conference (UPEC)},
doi = {10.1109/UPEC.2014.6934819},
file = {:home/alex/Downloads/06934819.pdf:pdf},
keywords = {ANN,ANN architecture heuristic tailoring,Artificial Neural Networks,Electric Load Forecasting,Forecasting,NEAT,Neuro-Evolution,Neuro-Evolution of Augmenting Topologies,Power Systems,Python,STFL,back propagation,backpropagation,electricity demand prediction,heuristic programming,load forecasting,neural nets,neuroevolution through augmenting topology,power engineering computing,short term electricity demand forecasting,short term load forecasting},
month = {sep},
pages = {1--5},
title = {{An investigation into using neuro-evolution of Augmenting Topologies (NEAT) for short term load forecasting (STFL)}},
year = {2014}
}
@article{Risi2010,
author = {Risi, Sebastian and Lehman, Joel and Stanley, Kenneth O},
file = {:home/alex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Risi, Lehman, Stanley - 2010 - Evolving the Placement and Density of Neurons in the HyperNEAT Substrate.pdf:pdf},
isbn = {9781450300728},
issn = {9781450300728},
journal = {GECCO '10 Proceedings of the 12th annual conference on Genetic and evolutionary computation},
keywords = {ES-HyperNEAT,hyperneat,neat,neuroevolution,substrate evolution},
number = {Gecco},
pages = {563--570},
title = {{Evolving the Placement and Density of Neurons in the HyperNEAT Substrate}},
year = {2010}
}
@article{Braun1993,
author = {Braun, H and Weisbrod, Joachim},
file = {:home/alex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Braun, Weisbrod - 1993 - Evolving neural networks for application oriented problems.pdf:pdf},
journal = {Proceedings of the Second Conference on Evolutionary Programming},
number = {January 2002},
pages = {62--71},
title = {{Evolving neural networks for application oriented problems}},
year = {1993}
}
@article{Tikhonov1963,
author = {Tikhonov, A N},
isbn = {0002-3264},
issn = {0002-3264},
journal = {Doklady Akademii Nauk SSSR, 151},
keywords = {Tikhonov},
number = {3},
pages = {501--504},
title = {{Solution of Incorrectly Formulated Problems And Regularization Method}},
volume = {151},
year = {1963}
}
@article{Kamruzzaman2002,
abstract = {Multilayer feedforward network trained by backpropagation$\backslash$nalgorithm suffers from slow learning speed. One of the reasons of slow$\backslash$nconvergence is the diminishing value of the derivative of the commonly$\backslash$nused activation functions as the nodes approaches saturated values. In$\backslash$nthis paper, we present a new activation function to accelerate$\backslash$nbackpropagation learning. A comparison among the commonly used$\backslash$nactivation functions, recently proposed logarithmic function and the$\backslash$nproposed activation function shows accelerated convergence with the$\backslash$nproposed one. This activation function can be used in conjunction with$\backslash$nother techniques to further accelerate the learning speed or reduce the$\backslash$nchance of being trapped in local minima. Simulation using this$\backslash$nactivation function shows improvement in the learning speed compared$\backslash$nwith other commonly used functions and the new activation function$\backslash$nproposed by Bilski (2000). This function may also be used in other$\backslash$nmultilayer feedforward training algorithms},
author = {Kamruzzaman, J. and Aziz, S.M.},
file = {:home/alex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kamruzzaman, Aziz - 2002 - A note on activation function in multilayer feedforward learning.pdf:pdf},
isbn = {0-7803-7278-6},
issn = {1098-7576},
journal = {Proceedings of the 2002 International Joint Conference on Neural Networks. IJCNN'02 (Cat. No.02CH37290)},
keywords = {activation,backpropagation learning,irnction,learning speed,neirral network},
title = {{A note on activation function in multilayer feedforward learning}},
volume = {1},
year = {2002}
}
@article{Gauci2010,
abstract = {Looking to nature as inspiration, for at least the past 25 years, researchers in the field of neuroevolution (NE) have developed evolutionary algorithms designed specifically to evolve artificial neural networks (ANNs). Yet the ANNs evolved through NE algorithms lack the distinctive characteristics of biological brains, perhaps explaining why NE is not yet a mainstream subject of neural computation. Motivated by this gap, this letter shows that when geometry is introduced to evolved ANNs through the hypercube-based neuroevolution of augmenting topologies algorithm, they begin to acquire characteristics that indeed are reminiscent of biological brains. That is, if the neurons in evolved ANNs are situated at locations in space (i.e., if they are given coordinates), then, as experiments in evolving checkers-playing ANNs in this letter show, topographic maps with symmetries and regularities can evolve spontaneously. The ability to evolve such maps is shown in this letter to provide an important advantage in generalization. In fact, the evolved maps are sufficiently informative that their analysis yields the novel insight that the geometry of the connectivity patterns of more general players is significantly smoother and more contiguous than less general ones. Thus, the results reveal a correlation between generality and smoothness in connectivity patterns. They also hint at the intriguing possibility that as NE matures as a field, its algorithms can evolve ANNs of increasing relevance to those who study neural computation in general.},
author = {Gauci, Jason and Stanley, Kenneth O},
file = {:home/alex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gauci, Stanley - 2010 - Autonomous evolution of topographic regularities in artificial neural networks.pdf:pdf},
journal = {Neural computation},
keywords = {0110,evolutionary computation,indirect encoding,neuroevolution,topographic maps},
mendeley-tags = {0110},
number = {7},
pages = {1860--1898},
title = {{Autonomous evolution of topographic regularities in artificial neural networks.}},
volume = {22},
year = {2010}
}
@book{Ariew1976,
address = {Champaign-Urbana},
author = {Ariew, Roger},
keywords = {rf},
publisher = {University of Illinois press},
title = {{Ockham's Razor: A Historical and Philosophical Analysis of Ockham's Principle of Parsimony}},
year = {1976}
}
@article{Chen2006,
abstract = {Appropriate topology and connection weight are two very important properties a neural network must have in order to successfully perform data classification. In this paper, we propose a hybrid training scheme Learning-NEAT (L-NEAT) for data classification problem. L-NEAT simplifies evolution by dividing the complete problem domain into sub tasks and learn the sub tasks by incorporating back propagation rule into the NeuroEvolution of Augmenting Topologies (NEAT) algorithm. The new algorithm combines the strength of searching for topology and weights from NEAT and back propagation respectively while overcoming problems associated with direct use of NEAT. We claim that L-NEAT can produce neural network for classification problem effectively and efficiently. Empirical evaluation shows that L-NEAT evolves classifying neural network with good generalization ability. Its accuracy outperforms original NEAT.},
author = {Chen, Lin and Alahakoon, Damminda},
doi = {10.1109/ICINFA.2006.374100},
file = {:home/alex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Alahakoon - 2006 - NeuroEvolution of augmenting topologies with learning for data classification.pdf:pdf},
isbn = {1424405556},
journal = {2nd International Conference on Information and Automation, ICIA 2006},
pages = {367--371},
title = {{NeuroEvolution of augmenting topologies with learning for data classification}},
year = {2006}
}
@article{MahsalKhan2013,
abstract = {A fast learning neuroevolutionary algorithm for both feedforward and recurrent networks is proposed. The method is inspired by the well known and highly effective Cartesian genetic programming (CGP) technique. The proposed method is called the CGP-based Artificial Neural Network (CGPANN). The basic idea is to replace each computational node in CGP with an artificial neuron, thus producing an artificial neural network. The capabilities of CGPANN are tested in two diverse problem domains. Firstly, it has been tested on a standard benchmark control problem: single and double pole for both Markovian and non-Markovian cases. Results demonstrate that the method can generate effective neural architectures in substantially fewer evaluations in comparison to previously published neuroevolutionary techniques. In addition, the evolved networks show improved generalization and robustness in comparison with other techniques. Secondly, we have explored the capabilities of CGPANNs for the diagnosis of Breast Cancer from the FNA (Finite Needle Aspiration) data samples. The results demonstrate that the proposed algorithm gives 99.5{\%} accurate results, thus making it an excellent choice for pattern recognitions in medical diagnosis, owing to its properties of fast learning and accuracy. The power of a CGP based ANN is its representation which leads to an efficient evolutionary search of suitable topologies. This opens new avenues for applying the proposed technique to other linear/non-linear and Markovian/non-Markovian control and pattern recognition problems. ?? 2013 Elsevier B.V.},
author = {{Mahsal Khan}, Maryam and {Masood Ahmad}, Arbab and {Muhammad Khan}, Gul and Miller, Julian F.},
doi = {10.1016/j.neucom.2013.04.005},
file = {:home/alex/Downloads/1-s2.0-S0925231213004499-main.pdf:pdf},
isbn = {0925-2312},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Artificial neural network,Breast cancer,Neuroevolution,Pole balancing,Recurrent networks},
pages = {274--289},
publisher = {Elsevier},
title = {{Fast learning neural networks using Cartesian genetic programming}},
url = {http://dx.doi.org/10.1016/j.neucom.2013.04.005},
volume = {121},
year = {2013}
}
@inproceedings{Mayer1993,
author = {Mayer, Helmut a and Schwaiger, Roland},
booktitle = {2002 International Joint Conference on Neural Networks Vol. 2},
file = {:home/alex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mayer, Schwaiger - 2002 - Differentiation of Neuron Types by Evolving Activation Function Templates for Artificial Neural Networks.pdf:pdf},
isbn = {0780372786},
pages = {1773--1778},
title = {{Differentiation of Neuron Types by Evolving Activation Function Templates for Artificial Neural Networks}},
year = {2002}
}
@article{Yao1999,
abstract = {Learning and evolution are two fundamental forms of adaptation. There has been a great interest in combining learning and evolution with artificial neural networks (ANNs) in recent years. This paper: 1) reviews different combinations between ANNs and evolutionary algorithms (EAs), including using EAs to evolve ANN connection weights, architectures, learning rules, and input features; 2) discusses different search operators which have been used in various EAs; and 3) points out possible future research directions. It is shown, through a considerably large literature review, that combinations between ANNs and EAs can lead to significantly better intelligent systems than relying on ANNs or EAs alone},
archivePrefix = {arXiv},
arxivId = {1108.1530},
author = {Yao, Xin},
doi = {10.1109/5.784219},
eprint = {1108.1530},
file = {:home/alex/Downloads/published{\_}iproc{\_}sep99.pdf:pdf},
isbn = {9780470287194},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {evolutionary computation,intelligent systems,neu-},
number = {9},
pages = {1423--1447},
pmid = {9821520},
title = {{Evolving artificial neural networks}},
volume = {87},
year = {1999}
}
@inproceedings{Ng2004,
author = {Ng, Andrew Y},
booktitle = {ICML},
file = {:home/alex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ng - 2004 - Feature selection, L1 vs. L2 regularization, and rotational invariance.pdf:pdf},
pages = {78--85},
title = {{Feature selection, L1 vs. L2 regularization, and rotational invariance}},
year = {2004}
}
